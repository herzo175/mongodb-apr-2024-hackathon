{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import tempfile\n",
    "\n",
    "from nomic import embed, login as nomic_login\n",
    "import ffmpeg\n",
    "import openai\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup clients\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"openai_api_key\"),\n",
    ")\n",
    "\n",
    "nomic_login(token=os.environ[\"nomic_api_key\"])\n",
    "mongo_client = pymongo.MongoClient(os.environ[\"MONGO_URI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with Apple clang version 14.0.0 (clang-1400.0.29.202)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/6.1.1_7 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopenvino --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inputs/video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "  Duration: 00:04:55.50, start: 0.000000, bitrate: 1339 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 1207 kb/s, 25 fps, 25 tbr, 12800 tbn (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 02/27/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 02/27/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> fps:default\n",
      "  fps:default -> Stream #0:0 (png)\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1181e0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1184d8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x118158000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x118168000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x118178000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x118188000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x118198000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1400a8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1400b8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1400c8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1382d0000] [swscaler @ 0x1400d8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x110698000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106a8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106b8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106c8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106d8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106e8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1106f8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x110708000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x110718000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x110728000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x118198000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x158008000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x160008000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x160018000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x131088000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x131098000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310a8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310b8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310c8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310d8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310e8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1481f8000] [swscaler @ 0x1310f8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x1501f0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150200000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150210000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150220000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150230000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150240000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150250000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150260000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150270000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150280000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x110728000] [swscaler @ 0x150290000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "Output #0, image2, to 'output_images/image_%04d.png':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/bt709/bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 2 fps, 2 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 png\n",
      "[out#0/image2 @ 0x12a86dbc0] video:573807kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
      "frame=  591 fps= 90 q=-0.0 Lsize=N/A time=00:04:55.00 bitrate=N/A speed=44.8x    \n"
     ]
    }
   ],
   "source": [
    "def video_to_images(video_path, output_path):\n",
    "    # Use ffmpeg to extract frames at 1 frame per second\n",
    "    (\n",
    "        ffmpeg.input(video_path)\n",
    "        .filter('fps', fps=2)\n",
    "        .output(output_path + '/image_%04d.png')\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "video_path = 'inputs/video.mp4'\n",
    "output_path = 'output_images'\n",
    "video_to_images(video_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'atlas_api_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m collection \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessi-video\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_images/*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, embed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      9\u001b[0m   imageData \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m:idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector-embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:embed}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/nomic/embed.py:176\u001b[0m, in \u001b[0;36mimages\u001b[0;34m(images, model)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# process any completed futures\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[0;32m--> 176\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m futures[future]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/nomic/embed.py:163\u001b[0m, in \u001b[0;36mimages.<locals>.send_request\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (i, response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/nomic/embed.py:121\u001b[0m, in \u001b[0;36mimages.<locals>.run_inference\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference\u001b[39m(batch):\n\u001b[1;32m    120\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m--> 121\u001b[0m         \u001b[43matlas_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matlas_api_path\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/embedding/image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    122\u001b[0m         headers\u001b[38;5;241m=\u001b[39matlas_class\u001b[38;5;241m.\u001b[39mheader,\n\u001b[1;32m    123\u001b[0m         data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-embed-vision-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    124\u001b[0m         files\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'atlas_api_path'"
     ]
    }
   ],
   "source": [
    "# create embeddings for images and add to MongoDB\n",
    "db = mongo_client[\"final-db\"]\n",
    "collection = db[\"messi-video\"]\n",
    "images = glob.glob(\"output_images/*.png\")\n",
    "\n",
    "image_embeddings = embed.images(images)\n",
    "\n",
    "for idx, embed in enumerate(image_embeddings['embeddings']):\n",
    "  imageData = {\"ts\":idx, \"vector-embedding\":embed}\n",
    "  collection.insert_one(imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index for embeddings\n",
    "collection.create_index([('vector-embedding', 'vector')], name='final_index', similarity=\"cosine\", numDimensions=768)\n",
    "\n",
    "# Embeddings as JSON:\n",
    "# {\n",
    "#   \"fields\": [\n",
    "#     {\n",
    "#       \"numDimensions\": 768,\n",
    "#       \"path\": \"vector-embedding\",\n",
    "#       \"similarity\": \"cosine\",\n",
    "#       \"type\": \"vector\"\n",
    "#     }\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subtitles\n",
    "def read_srt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "messi_transcript = read_srt(\"inputs/messi.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful video transcriber. You will be given the subtitles of a video and you need to convert it into text. Don't make things up. Just write what you hear. The user will provide you with the subtitles. Generate a summary of what is being said in the subtitles. The summary should not acknowledge the subtitles. Make sure to write in your own words and understand the context and meaning of the subtitles. Give out minimum 6 sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": messi_transcript}\n",
    "  ]\n",
    ")\n",
    "text_summary = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_embed = embed.text(texts=[text_summary], task_type=\"search_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video_clip(input_video_filename, output_video_filename, start_time, end_time):\n",
    "    ffmpeg.input(input_video_filename, ss=start_time, to=end_time).output(output_video_filename).run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_timestamps(input_video_filename, output_video_filename, timestamp_windows):\n",
    "    # create clips of video for timestamp windows and save to temp dir\n",
    "    # combine clips into video\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        # generate clips from source video by time stamp window\n",
    "        outfiles = []\n",
    "        \n",
    "        for window in timestamp_windows:\n",
    "            out = f\"{tempdir}/{str(uuid.uuid4())[:8]}.mp4\"\n",
    "\n",
    "            make_video_clip(input_video_filename, out, window[0], window[1])\n",
    "            outfiles.append(f\"file {out}\")\n",
    "\n",
    "        # Combine outfile paths into a txt file\n",
    "        combined_file = f\"{tempdir}/{str(uuid.uuid4())[:8]}.txt\"\n",
    "\n",
    "        with open(combined_file, \"w\") as fp:\n",
    "            fp.write(\"\\n\".join(outfiles))\n",
    "\n",
    "        # Combine clips using source files\n",
    "        ffmpeg.input(combined_file, format='concat', safe=0).output(output_video_filename, c='copy').run(overwrite_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"$vectorSearch\":\n",
    "    {\n",
    "        \"queryVector\": search_query_embed[\"embeddings\"][0],\n",
    "        \"path\": \"vector-embedding\",\n",
    "        \"numCandidates\": 100,\n",
    "        \"index\": \"final_index\",\n",
    "        \"limit\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [search_query]\n",
    "results = collection_imbeds.aggregate(pipeline)\n",
    "results_as_dict = list(results)\n",
    "timestamps = []\n",
    "for result in results_as_dict:\n",
    "    timestamps.append(result[\"ts\"])\n",
    "\n",
    "sT = sorted(timestamps)\n",
    "sT = [i/2 for i in sT]\n",
    "print(sT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "final_timestamps = []\n",
    "for timestamp in sT:\n",
    "    final_timestamps.append((str(datetime.timedelta(seconds=timestamp)), str(datetime.timedelta(seconds=timestamp + 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_timestamps(\"inputs/video.mp4\", \"outputs/out_demo.mp4\", final_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = open_ai_client.audio.speech.create(model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=text_summary\n",
    ")\n",
    "speech_file_path = \"outputs/speech.mp3\"\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_audio_video(input_video_file, input_audio_file, output_video_file):\n",
    "    input_video = ffmpeg.input(input_video_file)\n",
    "    input_audio = ffmpeg.input(input_audio_file)\n",
    "\n",
    "    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(output_video_file).run()\n",
    "\n",
    "combine_audio_video(\"outputs/out_demo.mp4\", speech_file_path, \"outputs/final_demo_with_ai_voice.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
