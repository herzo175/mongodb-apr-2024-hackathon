{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import tempfile\n",
    "\n",
    "from nomic import embed, login as nomic_login\n",
    "import ffmpeg\n",
    "import openai\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup clients\n",
    "load_dotenv(\"../backend/.env\")\n",
    "\n",
    "open_ai_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"openai_api_key\"),\n",
    ")\n",
    "\n",
    "nomic_login(token=os.environ[\"nomic_api_key\"])\n",
    "mongo_client = pymongo.MongoClient(os.environ[\"MONGO_URI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with Apple clang version 14.0.0 (clang-1400.0.29.202)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/6.1.1_7 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopenvino --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inputs/video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "  Duration: 00:04:55.50, start: 0.000000, bitrate: 1339 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 1207 kb/s, 25 fps, 25 tbr, 12800 tbn (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 02/27/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-02-28T07:06:13.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 02/27/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> fps:default\n",
      "  fps:default -> Stream #0:0 (png)\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118048000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118710000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118720000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118730000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118740000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118750000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118760000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118770000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118780000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x118790000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x1187a0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130af8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b08000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b18000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b28000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b38000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b48000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b58000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b68000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b78000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x130b88000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x128fd0000] [swscaler @ 0x128fe0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x128fd0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x128fe0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b88000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130af8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b08000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b18000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b28000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b38000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b48000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x130b58000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1187a0000] [swscaler @ 0x118048000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x1187a0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118048000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118710000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118720000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118730000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118740000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118750000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118760000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118770000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118780000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x1106f8000] [swscaler @ 0x118790000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "Output #0, image2, to 'output_images/image_%04d.png':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/bt709/bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 2 fps, 2 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 png\n",
      "[out#0/image2 @ 0x12270ead0] video:573807kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
      "frame=  591 fps= 87 q=-0.0 Lsize=N/A time=00:04:55.00 bitrate=N/A speed=43.5x    \n"
     ]
    }
   ],
   "source": [
    "def video_to_images(video_path, output_path):\n",
    "    # Use ffmpeg to extract frames at 1 frame per second\n",
    "    (\n",
    "        ffmpeg.input(video_path)\n",
    "        .filter('fps', fps=2)\n",
    "        .output(output_path + '/image_%04d.png')\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "video_path = 'inputs/video.mp4'\n",
    "output_path = 'output_images'\n",
    "video_to_images(video_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 3840, 'total_tokens': 3840}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n",
      "{'prompt_tokens': 16384, 'total_tokens': 16384}\n"
     ]
    }
   ],
   "source": [
    "# create embeddings for images and add to MongoDB\n",
    "db = mongo_client[\"final-db\"]\n",
    "collection = db[\"messi-video\"]\n",
    "images = glob.glob(\"output_images/*.png\")\n",
    "\n",
    "image_embeddings = embed.images(images)\n",
    "\n",
    "for idx, embed in enumerate(image_embeddings['embeddings']):\n",
    "  imageData = {\"ts\":idx, \"vector-embedding\":embed}\n",
    "  collection.insert_one(imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index for embeddings\n",
    "# collection.create_index([('vector-embedding', 'vector')], name='final_index', numDimensions=768)\n",
    "\n",
    "# Embeddings as JSON:\n",
    "# {\n",
    "#   \"fields\": [\n",
    "#     {\n",
    "#       \"numDimensions\": 768,\n",
    "#       \"path\": \"vector-embedding\",\n",
    "#       \"similarity\": \"cosine\",\n",
    "#       \"type\": \"vector\"\n",
    "#     }\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subtitles\n",
    "def read_srt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "messi_transcript = read_srt(\"inputs/messi.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful video transcriber. You will be given the subtitles of a video and you need to convert it into text. Don't make things up. Just write what you hear. The user will provide you with the subtitles. Generate a summary of what is being said in the subtitles. The summary should not acknowledge the subtitles. Make sure to write in your own words and understand the context and meaning of the subtitles. Give out minimum 6 sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": messi_transcript}\n",
    "  ]\n",
    ")\n",
    "text_summary = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_embed = embed.text(texts=[text_summary], task_type=\"search_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video_clip(input_video_filename, output_video_filename, start_time, end_time):\n",
    "    ffmpeg.input(input_video_filename, ss=start_time, to=end_time).output(output_video_filename).run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_timestamps(input_video_filename, output_video_filename, timestamp_windows):\n",
    "    # create clips of video for timestamp windows and save to temp dir\n",
    "    # combine clips into video\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        # generate clips from source video by time stamp window\n",
    "        outfiles = []\n",
    "        \n",
    "        for window in timestamp_windows:\n",
    "            out = f\"{tempdir}/{str(uuid.uuid4())[:8]}.mp4\"\n",
    "\n",
    "            make_video_clip(input_video_filename, out, window[0], window[1])\n",
    "            outfiles.append(f\"file {out}\")\n",
    "\n",
    "        # Combine outfile paths into a txt file\n",
    "        combined_file = f\"{tempdir}/{str(uuid.uuid4())[:8]}.txt\"\n",
    "\n",
    "        with open(combined_file, \"w\") as fp:\n",
    "            fp.write(\"\\n\".join(outfiles))\n",
    "\n",
    "        # Combine clips using source files\n",
    "        ffmpeg.input(combined_file, format='concat', safe=0).output(output_video_filename, c='copy').run(overwrite_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"$vectorSearch\":\n",
    "    {\n",
    "        \"queryVector\": search_query_embed[\"embeddings\"][0],\n",
    "        \"path\": \"vector-embedding\",\n",
    "        \"numCandidates\": 100,\n",
    "        \"index\": \"final_index\",\n",
    "        \"limit\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [search_query]\n",
    "results = collection_imbeds.aggregate(pipeline)\n",
    "results_as_dict = list(results)\n",
    "timestamps = []\n",
    "for result in results_as_dict:\n",
    "    timestamps.append(result[\"ts\"])\n",
    "\n",
    "sT = sorted(timestamps)\n",
    "sT = [i/2 for i in sT]\n",
    "print(sT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "final_timestamps = []\n",
    "for timestamp in sT:\n",
    "    final_timestamps.append((str(datetime.timedelta(seconds=timestamp)), str(datetime.timedelta(seconds=timestamp + 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_timestamps(\"inputs/video.mp4\", \"outputs/out_demo.mp4\", final_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = open_ai_client.audio.speech.create(model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=text_summary\n",
    ")\n",
    "speech_file_path = \"outputs/speech.mp3\"\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_audio_video(input_video_file, input_audio_file, output_video_file):\n",
    "    input_video = ffmpeg.input(input_video_file)\n",
    "    input_audio = ffmpeg.input(input_audio_file)\n",
    "\n",
    "    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(output_video_file).run()\n",
    "\n",
    "combine_audio_video(\"outputs/out_demo.mp4\", speech_file_path, \"outputs/final_demo_with_ai_voice.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
